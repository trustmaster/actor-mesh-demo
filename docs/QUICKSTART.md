# Actor Mesh Demo - Quick Start Guide

Get the E-commerce Support AI Agent running in under 5 minutes!

## ðŸš€ One-Line Setup

```bash
cd actor-mesh-demo && ./install.sh
```

This will automatically:
- âœ… Check Python 3.11+ installation
- âœ… Create virtual environment  
- âœ… Install all dependencies
- âœ… Start NATS and Redis servers (Docker)
- âœ… Run basic tests to verify setup

## ðŸ“‹ Prerequisites

- **Python 3.11+** (`python --version`)
- **Docker** (optional, for NATS/Redis)
- **LLM API Key** (optional, for full AI features):
  - OpenAI: `export OPENAI_API_KEY="your-key"`
  - Anthropic: `export ANTHROPIC_API_KEY="your-key"` 
  - Or run Ollama locally

## âš¡ Instant Demo

After installation, run the interactive demo:

```bash
# Activate environment (if not already active)
source venv/bin/activate  # Linux/Mac
# OR: venv\Scripts\activate  # Windows

# Run complete demo with 5 scenarios
python demo.py
```

**What you'll see:**
```
ðŸŽ¯ SCENARIO: Angry Customer - Delivery Issue
ðŸ’¬ Customer Message: "I am absolutely FURIOUS! My order ORD-12345678..."

ðŸŽ­ STEP 1: Sentiment Analysis
   âœ… Sentiment: negative (score: -0.8, confidence: 0.9)
   ðŸ“Š Urgency: high (score: 0.9)
   ðŸš¨ Is Complaint: True

ðŸ§  STEP 2: Intent Analysis  
   âœ… Intent: delivery_issue (confidence: 0.95)
   ðŸ·ï¸ Entities: order_number: "ORD-12345678"

ðŸ“‹ STEP 3: Context Retrieval
   ðŸ‘¤ Customer: John Doe (Premium tier)
   ðŸ“¦ Recent Orders: 3
   âš ï¸ Risk Factors: delivery_issues

âœï¸ STEP 4: Response Generation
   ðŸ’¬ "I sincerely apologize for the delivery delay..."
   ðŸŽ¬ Actions: [expedite_delivery, add_credit]

ðŸ›¡ï¸ STEP 5: Guardrail Validation
   âœ… Approved: True, Issues: 0

âš¡ STEP 6: Execution Coordination  
   âœ… Delivery expedited, $20 credit added
```

## ðŸ§ª Test Individual Components

Test actors without full pipeline:

```bash
# Test basic functionality (no external deps)
python test_basic_flow.py

# Test individual actors
python -m actors.sentiment_analyzer  # Ctrl+C to stop
python -m actors.intent_analyzer
python -m actors.response_generator
```

## ðŸ”§ Manual Setup (Alternative)

If automated installation fails:

```bash
# 1. Create virtual environment
python -m venv venv
source venv/bin/activate

# 2. Install dependencies  
pip install -r requirements.txt

# 3. Start NATS (required)
docker run -d -p 4222:4222 -p 8222:8222 --name nats-demo nats:latest -js

# 4. Start Redis (optional)
docker run -d -p 6379:6379 --name redis-demo redis:alpine

# 5. Configure environment
cp .env.example .env  # Edit with your API keys

# 6. Run demo
python demo.py
```

## ðŸŒ Start Mock APIs (Optional)

For full context retrieval, start mock services:

```bash
# Terminal 1 - Customer API
python -m mock_services.customer_api

# Terminal 2 - Orders API  
python -m mock_services.orders_api

# Terminal 3 - Tracking API
python -m mock_services.tracking_api
```

Access APIs at:
- Customer API: http://localhost:8001/docs
- Orders API: http://localhost:8002/docs  
- Tracking API: http://localhost:8003/docs

## ðŸ“Š Monitor System

While running, monitor:
- **NATS**: http://localhost:8222 (JetStream dashboard)
- **Logs**: Watch terminal output for message flow
- **Performance**: Demo shows timing for each step

## â“ Troubleshooting

### "Module not found" errors
```bash
# Ensure virtual environment is activated
source venv/bin/activate
# Verify installation
pip list | grep fastapi
```

### NATS connection errors  
```bash
# Check NATS is running
docker ps | grep nats
# Restart if needed
docker restart nats-demo
```

### LLM/API errors
```bash
# Set API key
export OPENAI_API_KEY="your-key-here"
# Or edit .env file
echo 'OPENAI_API_KEY=your-key' >> .env
```

### Tests fail
```bash
# Expected without LLM keys - core functionality still works
# Check specific errors in output
python test_basic_flow.py 2>&1 | grep -E "(ERROR|FAILED)"
```

## ðŸŽ¯ What's Working?

Even without LLM API keys, you'll see:
- âœ… **Sentiment Analysis**: Rule-based emotion detection
- âœ… **Message Routing**: Smart flow through actors  
- âœ… **Context Retrieval**: Customer data aggregation
- âœ… **Template Responses**: Fallback response generation
- âœ… **Guardrail Validation**: Safety and policy checks
- âœ… **Action Execution**: Simulated API operations

With LLM API keys, you get:
- ðŸ¤– **AI Intent Classification**: Advanced understanding
- ðŸ¤– **AI Response Generation**: Natural, context-aware responses
- ðŸ¤– **AI Guardrail Validation**: Intelligent safety checks

## ðŸš€ Next Steps

1. **Explore the Code**: Check `actors/` directory for implementations
2. **Modify Scenarios**: Edit `demo.py` to test your own messages  
3. **Add New Actors**: Follow the patterns in existing actors
4. **Deploy to Kubernetes**: Use `k8s/` manifests for production
5. **Integrate Real APIs**: Replace mock services with your systems

## ðŸ“š Learn More

- **Full Documentation**: See `README.md`
- **Architecture Details**: Read `spec/article.md`  
- **Implementation Guide**: Check `spec/implementation.md`
- **Project Summary**: Review `IMPLEMENTATION_SUMMARY.md`

## ðŸ†˜ Need Help?

- **Check Logs**: Most issues show clear error messages
- **Verify Prerequisites**: Ensure Python 3.11+ and Docker installed
- **Read Documentation**: Comprehensive guides available
- **Test Components**: Use `test_basic_flow.py` to isolate issues

---

**ðŸŽ‰ You're ready to explore Actor Mesh Architecture!**

The demo showcases a production-ready system handling real customer support scenarios with intelligent AI assistance, comprehensive safety checks, and robust error handling.